import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
import joblib

# Loading the dataset
df = pd.read_csv('ndtv_data_final.csv')

# Cleaning data: Handle missing values and select relevant features
df = df.dropna()
features = ['Brand', 'Battery capacity (mAh)', 'Screen size (inches)', 'Processor', 
            'RAM (MB)', 'Internal storage (GB)', 'Rear camera', 'Front camera']
X = df[features]
y = df['Price']

# Encoding categorical variables
label_encoder = LabelEncoder()


X.loc[:, 'Brand'] = label_encoder.fit_transform(X['Brand'])
X.loc[:, 'Processor'] = label_encoder.fit_transform(X['Processor'])


# Normalizing numerical features
scaler = StandardScaler()
numerical_features = ['Battery capacity (mAh)', 'Screen size (inches)', 'RAM (MB)', 
                      'Internal storage (GB)', 'Rear camera', 'Front camera']
X[numerical_features] = X[numerical_features].astype(float)
X.loc[:, numerical_features] = scaler.fit_transform(X[numerical_features])

# Saving the encoder and scaler
joblib.dump(label_encoder, 'label_encoder.pkl')
joblib.dump(scaler, 'scaler.pkl')

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Defining hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

# Initializing the model
rf = RandomForestRegressor(random_state=42)

# Performing Grid Search with 5-fold cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Getting the best model
best_model = grid_search.best_estimator_

# Saving the best model
joblib.dump(best_model, 'phone_price_model.pkl')

# Evaluating the best model
train_score = best_model.score(X_train, y_train)
test_score = best_model.score(X_test, y_test)
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Training R^2 Score: {train_score:.4f}")
print(f"Test R^2 Score: {test_score:.4f}")

# Feature importance
feature_importance = pd.Series(best_model.feature_importances_, index=features)
print("\nFeature Importance:")
print(feature_importance.sort_values(ascending=False))


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
import joblib

# Loading the dataset
df = pd.read_csv('ndtv_data_final.csv')

# Cleaning data: Handle missing values and select relevant features
df = df.dropna()
features = ['Brand', 'Battery capacity (mAh)', 'Screen size (inches)', 'Processor', 
            'RAM (MB)', 'Internal storage (GB)', 'Rear camera', 'Front camera']
X = df[features]
y = df['Price']

# Encoding categorical variables
label_encoder = LabelEncoder()
X['Brand'] = label_encoder.fit_transform(X['Brand'])
X['Processor'] = label_encoder.fit_transform(X['Processor'])

# Normalizing numerical features
scaler = StandardScaler()
numerical_features = ['Battery capacity (mAh)', 'Screen size (inches)', 'RAM (MB)', 
                      'Internal storage (GB)', 'Rear camera', 'Front camera']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

# Saving the encoder and scaler
joblib.dump(label_encoder, 'label_encoder.pkl')
joblib.dump(scaler, 'scaler.pkl')

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initializing the model with optimized hyperparameters
model = RandomForestRegressor(
    n_estimators=200,
    max_depth=20,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    random_state=42
)

# Training the model
model.fit(X_train, y_train)

# Saving the model
joblib.dump(model, 'phone_price_model.pkl')

# Evaluating the model
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)
print(f"Training R^2 Score: {train_score:.4f}")
print(f"Test R^2 Score: {test_score:.4f}")

# Feature importance
feature_importance = pd.Series(model.feature_importances_, index=features)
print("\nFeature Importance:")
print(feature_importance.sort_values(ascending=False))



